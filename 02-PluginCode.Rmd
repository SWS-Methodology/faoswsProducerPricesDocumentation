# **Imputation plugin and validation** {#Imputation}

After the outlier have been reviewed either for one or for all countries, the missing data imputation can be performed. The imputation of missing data first happens in the SWS via plugin and it is then assessed via shiny app.
The SWS plugin called 'PP_imputationMethods' estimate missing data according to up to six different methods and save the results in the 'imputation_annual_prices' datatable (Figure \@ref(fig:impDT)).

```{r  ImputationPluginWF, echo=FALSE, out.width="100%", fig.cap='Datatable compiled by the _PP_imputationMethods_ plugin for Burundi (M49 code 108) commodities wheat and maize (CPC code respectively 0111 and 0112).'}
knitr::include_graphics("img/ImputationPluginWF.png")
```


```{r  impDT, echo=FALSE, out.width="100%", fig.cap='Datatable compiled by the _PP_imputationMethods_ plugin for Burundi (M49 code 108) commodities wheat and maize (CPC code respectively 0111 and 0112).'}
knitr::include_graphics("img/ImpDT.png")
```

The 'imputation_annual_prices' datatable should not be reviewed by the user, all operations are to be performed in the shiny application.

The code for the 'PP_imputationMethods' plugin can be downloaded from the SWS at the plugin tab to check the current latest version. The plugin includes only one file.


## Plugin: PP_imputationMethods {#imputationMethods}



The 'PP_imputationMethods' plugin is run from the 'Annual Producer Prices (Preparation)' dataset and only has an optional country parameter as input. Through the country parameter, the user can select to run the imputation for one or a subset of countries, otherwise the plugin runs for all countries by default (Figure \@ref(fig:ImpPlugin1)).


```{r  ImpPlugin1, echo=FALSE, out.width="100%", fig.cap='How to run the PP_imputationMethods plugin from SWS interface.'}
knitr::include_graphics("img/ImpPlugin1.png")
```

When the plugin runs, it considers SLC series and completes the series where the last questionnaire year is missing according to the following methodologies: ARIMAX, Ensemble approach, Linear model, Commodity group approach, Consumer Price Index (CPI) based approach and Price ratio approach.

The auxiliary variables used for these models are:

- Value added of Agriculture, Forestry and Fishing (logarithmic form, from SWS Macro indicator domain)

- Country GDP (logarithmic form, from SWS Macro indicator domain)

- GDP per capita

- Gross fixed capital formation of agriculture, forestry and fishing (from published Capital Stock domain in FAOSTAT)

- Yield, i.e. harvested production per unit of harvested area for crop products (from SWS Agriculture production dataset)

- Trade openness index (calculated by the ECO team and stored in the datatable 'toi_data')

- Price ratios (as per datatable 'pp_tcf')


All imputation is performed considering the logarithmic value of the price in SLC.
Results of the plugin, i.e. prices estimated according to the methods listed above are stored in the datatable 'imputation_annual_prices'. These values are presented in the shiny application so the user is not required to check upon the datatable itself.

The first method calculated by the plugin is the price ratio. If available, the reference price is multiplied by the price ratio to calculate the missing price.

After the price ratio method all other approaches are implemented inside a loop. 
The loop consider separately each country-commodity combination.

1. ARIMAX: The key feature of the ARIMAX approach is the presence of covariates. The choice of covariates is performed empirically by running ARIMAX models for all possible combinations and choosing the one with the minimum Akaike Information Criteria (AIC) value. Once the model is selected the forecast is performed on the series and the price imputed.

2. Linear model: it follows the same approach of the ARIMAX approach but uses a simple linear regression to estimate and impute the missing price.

3. Ensemble approach: the ensemble method builds a collection of simple models and combine them to obtain a composite model. Averaging multiple models the risk of choosing a poor model is reduced. The approach has two main steps: the building of the models and their combination. The model considered are: mean (mean of all observations), median (median of all observations), linear (linear regression), exponential (exponential function), logistic (logistic function), naive (linear interpolation followed by last observation carried forward and first observation carried backward), ARIMA (selected based on the corrected Akaikeâ€™s Information Criterion (AICC) and imputation via Kalman Filter), LOESS (local regression with linear models and model window varying based on sample size), splines(cubic spline interpolation), MARS (Multivariate Adaptive Regression Spline), mixed model (linear mixed model with time as a fixed effect and country as the random effect). All these models are then combined so that the final result is a weighted average of all the input models. This approach is completely automatized by the R function in the R package faoswsImputation. An extended presentation is available in the faoswsImputation package along with the function used to impute variable in the Git Hub repository SWS-Methodology.


4. Commodity group approach: starting from the CPC hierarchical classification, the higher level of the considered product is taken and all products belonging to this level are considered. The growth rate of each product is computed and the median of the group is calculated. The missing price value is then imputed applying the median growth rate to the latest available price. Depending on the availability of prices, the plugin consider two different level of the hierarchy.


5. Consumer Price Index (CPI) approach: CPI values are pulled from the SWS dataset 'consumer_price_indices'. If available, the food CPI is considered otherwise the general CPI is used. The growth rate is computed and applied to the latest price available to calculate the missing value.


6. Gross Domestic Price (GDP) approach: this approach considers either the value added of agriculture, forestry and fishing or the GDP if the previous is not available. As in previous approaches the growth rate is calculated and applied to the latest price available.


All these possible imputation values are stored in the 'imputation_annual_prices' datatable and are displayed as possible values in the shiny application.


There is a second part of the plugin that performs interpolation. The interpolation is perfomed in case an official value has been collected through questionnaire or other publications in the latest year but the previous years had been imputed. To check if previous imputations were appropriate values are re-calculated through interpolation and proposed to the suer through the shiny app. Interpolation is performed through a kalman filter on the best^{Best according to AIC as in the imputaiton part.} ARIMA model available or through a cubic spline interpolation. The spline approach is used if the ARIMA imputaiton provide no result or the estimated price is considered as an outlier, i.e. is above (below) the median price plus (minus) two standard deviations of the available series.
The interploated values are saved in the 'interpolation_annual_prices' datatable and presented to the user in the shiny app.


### Plugin code

```
# -- Load Packages ----

suppressMessages({
  library(data.table)
  library(DT)
  library(forecast)
  library(tseries)
  library(faosws)
  library(faoswsFlag)
  library(faoswsProcessing)
  library(faoswsUtil)
  library(faoswsImputation)
  library(imputeTS)
  library(ggplot2)
  library(sendmailR)
})

# -- Token QA ----


if(CheckDebug()){
  library(faoswsModules)
  SETTINGS = ReadSettings("sws.yml")
  R_SWS_SHARE_PATH = SETTINGS[["share"]]
  SetClientFiles(SETTINGS[["certdir"]])
  GetTestEnvironment(baseUrl = SETTINGS[["server"]],
                     token = SETTINGS[["token"]])#'4e9d9a2e-5258-48b6-974f-3656d1af8217')
}

# -- Expand Year ----
message('PP_imputationMethods: initiate plugin')

expandYear <- function (data, areaVar = "geographicAreaM49", elementVar = "measuredElement", 
                        itemVar = "measuredItemCPC", yearVar = "timePointYears", 
                        valueVar = "Value", obsflagVar = "flagObservationStatus", 
                        methFlagVar = "flagMethod", newYears = NULL) 
{
  key = c(elementVar, areaVar, itemVar)
  keyDataFrame = data[, key, with = FALSE]
  keyDataFrame = keyDataFrame[with(keyDataFrame, order(get(key)))]
  keyDataFrame = keyDataFrame[!duplicated(keyDataFrame)]
  yearDataFrame = unique(data[, get(yearVar)])
  if (!is.null(newYears)) {
    yearDataFrame = unique(c(yearDataFrame, newYears, newYears - 
                               1, newYears - 2))
  }
  yearDataFrame = data.table(yearVar = yearDataFrame)
  colnames(yearDataFrame) = yearVar
  completeBasis = data.table(merge.data.frame(keyDataFrame, 
                                              yearDataFrame))
  expandedData = merge(completeBasis, data, by = colnames(completeBasis), 
                       all.x = TRUE)
  expandedData = fillRecord(expandedData, areaVar = areaVar, 
                            itemVar = itemVar, yearVar = yearVar,
                            flagObsVar = obsflagVar, 
                            flagMethodVar = methFlagVar)
  seriesToBlock = expandedData[(get(methFlagVar) != "u"), ]
  seriesToBlock[, `:=`(lastYearAvailable, max(get(yearVar))), 
                by = key]
  seriesToBlock[, `:=`(flagComb, paste(get(obsflagVar), get(methFlagVar), 
                                       sep = ";"))]
  seriesToBlock = seriesToBlock[get(yearVar) == lastYearAvailable & 
                                  flagComb == "M;-"]
  if (nrow(seriesToBlock) > 0) {
    seriesToBlock = seriesToBlock[, {
      max_year = max(as.integer(.SD[, timePointYears]))
      data.table(timePointYears = seq.int(max_year + 1, 
                                          newYears), Value = NA_real_, flagObservationStatus = "M", 
                 flagMethod = "-")[max_year < newYears]
    }, by = key]
    expandedData = merge(expandedData, seriesToBlock, by = c(areaVar, 
                                                             elementVar, itemVar, yearVar), all.x = TRUE, suffixes = c("", 
                                                                                                                       "_MDash"))
    expandedData[!is.na(flagMethod_MDash), `:=`(flagMethod, 
                                                flagMethod_MDash)]
    expandedData = expandedData[, colnames(data), with = FALSE]
  }
  expandedData
}

# -- Pull preparation data SLC ----
countryPar <- swsContext.computationParams$countries

message('PP_imputationMethods: get data')

domainPP <- 'prod_prices'
datasetVal <- 'annual_producer_prices_validated'
datasetPrep <- 'annual_producer_prices_prep' 
SLCelement <- '5531'


if(!is.null(countryPar) & length(countryPar) > 0){
  countryPar <- swsContext.computationParams$countries
  sessionCountry <- strsplit(countryPar, ', ')[[1]]
} else {
  sessionCountry <- swsContext.datasets[[1]]@dimensions$geographicAreaM49@keys
  countries <- GetCodeList(domainPP, datasetPrep, "geographicAreaM49")[ type == 'country']$code
  # Make sure only countries not areas
  sessionCountry <- sessionCountry[sessionCountry %in% countries]
}

message(paste("Prod Prices: countries selected ", paste0(sessionCountry, collapse = ', '), '.', sep = ''))


lastyear <- as.character(as.numeric(format(Sys.Date(), '%Y'))-2)

preppriceKey = DatasetKey(
  domain = domainPP,
  dataset = datasetPrep,
  dimensions = list(
    Dimension(name = "geographicAreaM49",
              keys = sessionCountry),
    Dimension(name = "measuredElement", 
              keys = SLCelement),
    Dimension(name = "measuredItemCPC",
              keys = GetCodeList('agriculture', 'aproduction', 'measuredItemCPC')[, code]),
    Dimension(name = "timePointYears", 
              keys = GetCodeList(domainPP, datasetPrep, 'timePointYears')[code > as.character(as.numeric(lastyear)-20), code]))
  
)

prep_price0 <- GetData(preppriceKey, flags = TRUE)
impYear <- max(as.numeric(unique(prep_price0$timePointYears)))
# Impute last three years, i.e. delete previous imputations (timePointYears %in% as.character((impYear-2):impYear))
# or only last year (timePointYears == impYear)
prep_price0[!flagObservationStatus %in% c('', 'X') & 
              timePointYears == impYear,
            c('Value', 'flagObservationStatus', 'flagMethod') := 
              list(NA, 'M', 'u')]

# Get space for imputations
prep_price <- expandYear(prep_price0, newYears = impYear)
# If one year not in the selected ones is missing the system ignore them
prep_price <- prep_price[!prep_price[timePointYears != impYear & 
                                       flagMethod == 'u'], on = names(prep_price)]

# -- Auxiliary variable ----

message('PP_imputationMethods: add covariates')

# Macro Indicators
GDP_VAcode <- c('8005', '8028') # GDP and VA codes

MIKey = DatasetKey(
  domain = 'macro_stats',
  dataset = 'ess_eco_macroind_complete',
  dimensions = list(
    Dimension(name = "geographicAreaM49",
              keys = GetCodeList('macro_stats', 
                                 'ess_eco_macroind_complete', 
                                 'geographicAreaM49')[code %in% sessionCountry, code]),
    Dimension(name = "measuredElementGdp", 
              keys = GetCodeList('macro_stats', 
                                 'ess_eco_macroind_complete', 
                                 'measuredElementGdp')[code %in% GDP_VAcode, code]),
    Dimension(name = "timePointYears", 
              keys = GetCodeList('macro_stats', 
                                 'ess_eco_macroind_complete', 
                                 'timePointYears')[code %in% unique(prep_price$timePointYears), code]))
)

macro_ind <- GetData(MIKey, flags = TRUE)
# Get 2 separated columns
macro_ind <- dcast(macro_ind, geographicAreaM49 + timePointYears ~ measuredElementGdp, 
                   value.var = 'Value' )

setnames(macro_ind, c('8005', '8028'), c('ValueGDP', 'ValueVA'))

GDPapproach <- copy(macro_ind)
GDPapproach[!is.na(ValueVA), GDP2use := ValueVA]
GDPapproach[is.na(ValueVA), GDP2use := ValueGDP]
GDPapproach[,c('ValueGDP', 'ValueVA')] <- NULL

prep12 <- merge(prep_price, macro_ind, by = c("geographicAreaM49","timePointYears"),
                all.x = T)

prep12[!is.na(ValueGDP) , LogGDP := log(ValueGDP)]
prep12[!is.na(ValueVA) , LogVA := log(ValueVA)]


# Yield
yieldcode <- '5421'

yieldKey = DatasetKey(
  domain = 'agriculture',
  dataset = 'aproduction',
  dimensions = list(
    Dimension(name = "geographicAreaM49",
              keys = GetCodeList('agriculture', 
                                 'aproduction', 
                                 'geographicAreaM49')[code %in% sessionCountry, code]),
    Dimension(name = "measuredElement", 
              keys = GetCodeList('agriculture', 
                                 'aproduction', 
                                 'measuredElement')[code == yieldcode, code]),
    Dimension(name = "measuredItemCPC",
              keys = GetCodeList('agriculture', 'aproduction', 'measuredItemCPC')[, code]),
    Dimension(name = "timePointYears", 
              keys = GetCodeList('agriculture', 
                                 'aproduction', 
                                 'timePointYears')[code %in% unique(prep_price$timePointYears), code]))
)

yield <- GetData(yieldKey, flags = TRUE)
setnames(yield, 'Value', 'ValueYield')
yield[, c("measuredElement", "flagObservationStatus", "flagMethod")] <- NULL

prep123 <- merge(prep12, yield, 
                 by = c("geographicAreaM49", 
                        "timePointYears",
                        "measuredItemCPC"), all.x = T)

# TOI

toi <- ReadDatatable('toi_data')
prepcov <- merge(prep123, toi, by.x = c("geographicAreaM49", 
                                        "timePointYears"),
                 by.y = c("geographicaream49", 
                          "timepointyears"), all.x = T)




# -- Missing value ----
prepcov <- prepcov[order(timePointYears)]
prepcov[!is.na(Value), LogValue := log(Value)]

# series with missing data

geocommmiss <- unique(prepcov[flagObservationStatus == 'M' & flagMethod == 'u', .(geographicAreaM49, measuredItemCPC)])


# -- Applying Price ratios (Value_PR) ----

tcf <- ReadDatatable('pp_tcf')
setnames(tcf, c("country_code", "cpc2convert"), 
         c('geographicAreaM49', 'measuredItemCPC'))

prepcov <- merge(prepcov, tcf[,c('geographicAreaM49', 'measuredItemCPC',
                                 "cpc_reference", 'tcf'), with = F],
                 by = c('geographicAreaM49', 'measuredItemCPC'), all.x = T)

tcfneed <- prepcov[!is.na(tcf) & is.na(Value) & flagObservationStatus == 'M' & flagMethod == 'u']
refneed <- unique(tcfneed[,.(geographicAreaM49, cpc_reference, timePointYears)])
refneed <- merge(refneed, prepcov[,.(geographicAreaM49,
                                     measuredItemCPC,
                                     timePointYears, Value)], 
                 by.x = c('geographicAreaM49', 'cpc_reference', 'timePointYears'), 
                 by.y = c('geographicAreaM49', 'measuredItemCPC', 'timePointYears'),
                 all.x = T)
refneed <- refneed[!is.na(Value)]
convprices <- merge(prepcov, refneed, 
                    by =  c('geographicAreaM49', 'cpc_reference', 'timePointYears'),
                    all.x = T, suffixes = c('', '_ref'))

class(convprices$Value_ref)
convprices[is.na(Value) & 
             !is.na(tcf) & 
             !is.na(Value_ref) & 
             flagObservationStatus == 'M' & flagMethod == 'u', 
           PriceRatio := tcf*Value_ref]

convprices[,c("cpc_reference", "tcf", "Value_ref")] <- NULL

prepcov <- convprices

# -- Start loop ----

message('PP_imputationMethods: start imputation')

cpchierarchy <- ReadDatatable('cpc_hierarchy_ebx5')

fulldata <- data.table()

for(geo in unique(geocommmiss$geographicAreaM49)){
  series_geo <- prepcov[geographicAreaM49 == geo]
  
  for(selComm in unique(geocommmiss[geographicAreaM49 == geo]$measuredItemCPC)){
    
    series_comm <- series_geo[measuredItemCPC == selComm]
    series_comm <- series_comm[,colSums(is.na(series_comm))<nrow(series_comm), with = F]
    
    # For LM
    series_comm_lm <- copy(series_comm)
    series_comm_lm$timePointYears <- as.numeric(series_comm_lm$timePointYears)
    xreg_comm <- series_comm[,names(series_comm) %in% c("timePointYears", "geographicAreaM49", "measuredItemCPC",    
                                                        "LogGDP", "LogVA", "ValueYield", "toi_aff"), with = F] # , "TOI_AFF"
    
    xreg_comm <- xreg_comm[,colSums(is.na(xreg_comm))<nrow(xreg_comm), with = F]
    xreg_commsig <- copy(xreg_comm)
    
    tryCatch({
      # -- Variable selection ----
      
      ppseries <- ts(series_comm$LogValue, start = min(as.numeric(series_comm$timePointYears)))
      
      # Covariates available
      vec2comb <- names(xreg_comm)[4:ncol(xreg_comm)]
      
      # Create a list with possible covariate combinations
      dfcomb <- list()
      
      for(h in 1:length(vec2comb)){
        dfcomb[[h]] <- as.data.frame(t(combn(vec2comb, h)))
      }
      
      covcomb <- rbindlist(dfcomb, use.names = T, fill = T)  
      
      # Models using the possible combinations in covcomb
      modlist <- list()
      lmlist <- list()
      
      for(j in 1:nrow(covcomb)){
        cov2use <- as.matrix(covcomb[j,])
        xreg2use <- xreg_comm[ , cov2use[!is.na(cov2use)], with = F]
        modj <- auto.arima(ppseries, seasonal = FALSE, xreg = xreg2use)
        
        modlist[[j]] <- modj
        
        formula <- as.formula(paste('LogValue ~ timePointYears + ',
                                    paste(names(xreg2use), collapse = ' + '), collapse = ''))
        lmlist[[j]] <- lm(formula = formula, data = series_comm_lm)
      }
      
      # Best ARIMAX with lower AIC
      bestmod <- modlist[[which(unlist(lapply(modlist, function(x){x$aic})) ==
                                  min(unlist(lapply(modlist, function(x){x$aic}))))]]
      
      # Best LM with lowest AIC
      AICnotInf <- unlist(lapply(lmlist, function(x){AIC(x)}))
      AICnotInf <- AICnotInf[AICnotInf != -Inf]
      AICnotInf <- AICnotInf[AICnotInf != Inf]
      bestlm <- lmlist[[which(unlist(lapply(lmlist, function(x){AIC(x)})) == 
                                min(AICnotInf))]]
      
      
      nh <- impYear - max(as.numeric(series_comm[!is.na(LogValue)]$timePointYears))
      
      #-- ARIMA forescast ----
      # First option
      # Covariates for predictive years
      varselected <- names(bestmod$coef)[names(bestmod$coef) %in% names(xreg_comm)]
      xreg_comm_pred <- xreg_commsig[(nrow(xreg_commsig)-nh+1):nrow(xreg_commsig),
                                     varselected, with = F]
      
      pred <- forecast(bestmod, h = nh, xreg = xreg_comm_pred)$mean
      
      #-- Linear model ----
      series_comm_lm_pred <- copy(series_comm_lm)
      series_comm_lm_pred <- series_comm_lm_pred[(nrow(series_comm)-nh+1):nrow(series_comm)]
      series_comm_lm_pred$timePointYears <- as.numeric(series_comm_lm_pred$timePointYears)
      
      predlm <- forecast(bestlm, h = nh, series_comm_lm_pred)$mean
      
      #-- Kalman ----
      
      # predKalm <- na.kalman(x = ppseries, model = 'auto.arima', xreg = xreg_commsig[, varselected, with = F])
      
      # -- Ensemble approach ----
      impPar <- defaultImputationParameters()
      impPar$imputationValueColumn="Value"
      impPar$imputationFlagColumn="flagObservationStatus"
      impPar$imputationMethodColumn="flagMethod"
      impPar$byKey=c("geographicAreaM49", "measuredItemCPC")
      impPar$estimateNoData=FALSE
      
      # If the data series contains only zero and missing value then it is considered to contain no information for imputation.
      
      pp_ensemble <- removeNoInfo(series_comm,
                                  value="Value",
                                  observationFlag = "flagObservationStatus",
                                  byKey = c(impPar$byKey))
      
      pp_ensemble_sub <- pp_ensemble[,c("geographicAreaM49",
                                        "timePointYears",
                                        "measuredItemCPC",
                                        "Value",
                                        "flagObservationStatus",
                                        "flagMethod"), with = F]
      
      # If no missing data the commodityDB does not change
      pp_ensemble_imp <- imputeVariable(data = pp_ensemble_sub,
                                        imputationParameters = impPar)
      
      predEns <- pp_ensemble_imp$Value
      
      # -- Commodity group information ----
      
      hierarchyComm <- cpchierarchy[apply(cpchierarchy, 1, function(r) any(r %in% selComm))]
      
      colComm <-  names(hierarchyComm)[
        names(hierarchyComm)== 
          names(hierarchyComm)[(apply(cpchierarchy, 2, 
                                      function(r) any(r %in% selComm)))]]
      
      if(colComm == "code_l4"){
        hier1 <- unique(hierarchyComm[ , "code_l3", with = F]) 
        codes2compare <- c(cpchierarchy[code_l3 == hier1$code_l3 & !is.na(code_l4), ]$code_l4,
                           cpchierarchy[code_l3 == hier1$code_l3 & !is.na(code_l5), ]$code_l5,
                           cpchierarchy[code_l3 == hier1$code_l3 & !is.na(code_l6), ]$code_l6)
        
        
        
      } else {
        hier1 <- unique(hierarchyComm[ , "code_l4", with = F])
        codes2compare <- c(cpchierarchy[code_l4 == hier1$code_l4 & !is.na(code_l5), ]$code_l5,
                           cpchierarchy[code_l4 == hier1$code_l4 & !is.na(code_l6), ]$code_l6)
      }
      
      commoditygroup <- series_geo[!is.na(Value) &
                                     measuredItemCPC %in% codes2compare]
      
      # If there are data ok otherwise hierarchy up (if code_l4)
      if(nrow(commoditygroup[timePointYears %in% 
                             as.character(impYear)]) == 0
         & names(hier1) == 'code_l4'){
        
        hier1 <- unique(hierarchyComm[ , "code_l3", with = F]) 
        codes2compare <- c(cpchierarchy[code_l3 == hier1$code_l3 & !is.na(code_l4), ]$code_l4,
                           cpchierarchy[code_l3 == hier1$code_l3 & !is.na(code_l5), ]$code_l5,
                           cpchierarchy[code_l3 == hier1$code_l3 & !is.na(code_l6), ]$code_l6)
        
        commoditygroup <- series_geo[!is.na(Value) &
                                       measuredItemCPC %in% codes2compare]
        
      }else if(nrow(commoditygroup[timePointYears %in% 
                                   as.character((impYear-2):impYear)]) == 0
               & names(hier1) == 'code_l3'){
        print('Commodity group approch not applicable') #Should be an NA in the final file just to have all results 
      }
      
      # calculate growth rate and the mean or median applied to product
      commoditygroup$timePointYears <- as.numeric(commoditygroup$timePointYears)
      commoditygroup <- commoditygroup[order(timePointYears)]
      # commoditygroup[, cgGR := diff(Value)/shift(Value)]
      
      commoditygroup[ , cgGR := c(NA, diff(Value))/shift(Value),
                      by = c("geographicAreaM49",
                             "measuredElement",
                             "measuredItemCPC")]
      
      commoditygroup[ , c('meanbyyear',
                          'medianbyyear'):= list(mean(cgGR, na.rm = T),
                                                 median(cgGR, na.rm = T)),
                      by = 'timePointYears']
      
      commoditygroup2merge <- unique(commoditygroup[,.(timePointYears, medianbyyear)])
      commoditygroup2merge$timePointYears <- as.character(commoditygroup2merge$timePointYears)
      
      years <- series_comm[flagMethod == 'u']$timePointYears
      cgmethod <- copy(series_comm)
      
      cgmethod <- merge(cgmethod, commoditygroup2merge,
                        by = 'timePointYears', all.x = T)
      cgmethod <- cgmethod[order(timePointYears)]
      cgmethod[, ValueCG := shift(Value)*(1+medianbyyear)]
      cgmethod[is.na(Value), Value := ValueCG]               
      
      # -- CPI ----
      
      cpiKey = DatasetKey(
        domain = domainPP,
        dataset = 'consumer_price_indices',
        dimensions = list(
          Dimension(name = "geographicAreaM49",
                    keys = GetCodeList(domainPP, 'consumer_price_indices', 'geographicAreaM49')[code %in% sessionCountry, code]),
          Dimension(name = "measuredElement", 
                    keys = GetCodeList(domainPP, 'consumer_price_indices', 'measuredElement')[code %in% c('23012', '23013'), code]),
          Dimension(name = "timePointYears",
                    keys = GetCodeList(domainPP, 'consumer_price_indices', 'timePointYears')[, code]),
          Dimension(name = "timePointMonths", 
                    keys = GetCodeList(domainPP, 'consumer_price_indices', 'timePointMonths')[code == '7013', code]))
      )
      
      cpi0 <- GetData(cpiKey, flags = TRUE)
      cpi0[, timePointMonths := NULL]
      cpi0 <- cpi0[order(as.numeric(timePointYears))]
      cpi0[ , GR := c(NA, diff(Value))/shift(Value),
            by = c('geographicAreaM49', 'measuredElement')]
      cpi <- dcast(cpi0, geographicAreaM49 + timePointYears ~ measuredElement, value.var = 'GR')
      setnames(cpi, c("23012",  "23013"), c("GeneralCPI", "FoodCPI"))
      
      cpi[!is.na(FoodCPI) , GR2use := FoodCPI]
      cpi[is.na(FoodCPI) , GR2use := GeneralCPI]
      
      cpimethod <- copy(series_comm)
      
      cpimethod <- merge(cpimethod, cpi[,.(geographicAreaM49, 
                                           timePointYears,
                                           GR2use)],
                         by = c('geographicAreaM49', 'timePointYears'), all.x = T)
      cpimethod[order(timePointYears)]
      cpimethod[, ValueCPI := shift(Value)*(1+shift(GR2use))]
      cpimethod[is.na(Value), Value := ValueCPI]      
      
      # -- GDP or AgGDP ----
      gdpmethod <- copy(series_comm)
      GDPapproach[ , GRgdp := c(NA, diff(GDP2use))/shift(GDP2use)]
      
      gdpmethod <- merge(gdpmethod, GDPapproach, 
                         by = c('geographicAreaM49', 'timePointYears'), 
                         all.x = T)
      
      gdpmethod[order(timePointYears)]
      gdpmethod[, ValueGDPdefl := shift(Value)*(1+GRgdp)]
      gdpmethod[is.na(Value), Value := ValueGDPdefl]  
      
      # -- AgPPI ----
      
      # -- End of imputation methods ----
      
      newimputation <- series_comm[, names(prep_price), with = F]
      newimputation[ , c('ARIMAX',
                         # 'Kalman', 
                         'Ensemble',
                         'LM',
                         'Comm_Group',
                         'CPI',
                         'PriceRatio',
                         'GDP') := list(c(Value[!is.na(Value)], exp(pred)),
                                        #    exp(predKalm),
                                        predEns,
                                        c(Value[!is.na(Value)], exp(predlm)),
                                        cgmethod$Value,
                                        cpimethod$Value,
                                        convprices[geographicAreaM49 == geo &
                                                     measuredItemCPC == selComm]$PriceRatio,
                                        gdpmethod$Value)] 
      
      fulldata <- rbind(fulldata, newimputation[flagMethod == 'u'])
      
    }, error=function(e){})
    
    #print(selComm)
  }
  
}

message('PP_imputationMethods: save imputation')

imp2save <- copy(fulldata[,!names(fulldata) %in% c("measuredElement",
                                                   "Value",
                                                   "flagObservationStatus",
                                                   "flagMethod"), with = F])
imp2save <- melt(imp2save,
                 id.vars = c("geographicAreaM49",
                             "measuredItemCPC",
                             "timePointYears"),
                 measure.vars = 4:ncol(imp2save),
                 variable.name = 'approach',
                 value.name = 'estimation')
names(imp2save) <- tolower(names(imp2save))

changeset <- Changeset('imputation_annual_prices')
currentImp <- ReadDatatable('imputation_annual_prices', readOnly = FALSE)

if(nrow(currentImp[geographicaream49 %in% sessionCountry])>0){
  AddDeletions(changeset,currentImp[geographicaream49 %in% sessionCountry])
  Finalize(changeset)
}
AddInsertions(changeset, imp2save)
Finalize(changeset)

#---- Interpolation ----

message('PP_imputationMethods: start interpolation')

interpolated <- data.table()

#prepcov[timePointYears == impYear & flagObservationStatus == '']
#Check data with offcial last year figures
checkinterp <- unique(prepcov[timePointYears == impYear & flagObservationStatus %in% c('', 'X'), .(geographicAreaM49, measuredItemCPC)])

interDTcheck <- prepcov[prepcov[checkinterp, on = c('geographicAreaM49', 'measuredItemCPC'), which=TRUE]]

# Previous year must be imputed otherwise no interpolation
checkingDT <- interDTcheck[timePointYears == as.character(as.numeric(impYear)-1) & !flagObservationStatus %in% c('', 'X')]
tochange <- interDTcheck[interDTcheck[checkingDT, on  = c('geographicAreaM49', 'measuredItemCPC'), which = TRUE]]

# Last official value as minimum start year for interpolation
tochange[timePointYears != impYear & flagObservationStatus %in% c('', 'X'), 
         minyear := max(timePointYears), by =  c('geographicAreaM49', 'measuredItemCPC')]
# Assign minyear value also to last year
# tochange[timePointYears == impYear, minyear := max(timePointYears)]
# Only consider series where maximum three years need to be interpolated
tochange <- tochange[ minyear >= (impYear-4)]

geocominterp <- unique(tochange[,.(geographicAreaM49, measuredItemCPC)])

# Take data only for these combinations
data2interp <- merge(prepcov, geocominterp, by = c('geographicAreaM49', 'measuredItemCPC'))
data2interp[, LogValueInterp := LogValue]
data2interp[!flagObservationStatus %in% c('', 'X') & timePointYears >= (impYear-4), LogValueInterp := NA]

for(geoint in unique(geocominterp$geographicAreaM49)){
  series_geoint <- data2interp[geographicAreaM49 == geoint]
  
  for(comint in unique(geocominterp[geographicAreaM49 == geoint]$measuredItemCPC)){
    
    series_comint <- series_geoint[measuredItemCPC == comint]
    series_comint <- series_comint[,colSums(is.na(series_comint))< nrow(series_comint), with = F]
    series_comint <- series_comint[,apply(series_comint,2,function(series_comint) !all(series_comint==0)), with = F]
    
    xreg_comint <- series_comint[,names(series_comint) %in% c("timePointYears", "geographicAreaM49", "measuredItemCPC",    
                                                              "LogGDP", "LogVA", "ValueYield", "toi_aff"), with = F] # , "TOI_AFF"
    
    xreg_comint <- xreg_comint[,colSums(is.na(xreg_comint))<nrow(xreg_comint), with = F]
    xreg_comintsig <- copy(xreg_comint)
    
    # -- Variable selection ----
    
    ppseries <- ts(series_comint$LogValueInterp, start = min(as.numeric(series_comint$timePointYears)))
    
    # Covariates available
    vec2comb <- names(xreg_comint)[4:ncol(xreg_comint)]
    
    # Create a list with possible covariate combinations
    dfcomb <- list()
    
    for(h in 1:length(vec2comb)){
      dfcomb[[h]] <- as.data.frame(t(combn(vec2comb, h)))
    }
    
    covcomb <- rbindlist(dfcomb, use.names = T, fill = T)
    
    # Models using the possible combinations in covcomb
    modlist <- list()
    
    for(j in 1:nrow(covcomb)){
      cov2use <- as.matrix(covcomb[j,])
      xreg2use <- xreg_comint[ , cov2use[!is.na(cov2use)], with = F]
      modj <- auto.arima(ppseries, seasonal = FALSE, xreg = xreg2use)
      
      modlist[[j]] <- modj
      
    }
    
    # Best ARIMAX with lower AIC
    bestmod <- modlist[[which(unlist(lapply(modlist, function(x){x$aic})) ==
                                min(unlist(lapply(modlist, function(x){x$aic}))))]]
    
    nh <- impYear - max(as.numeric(series_comint[!is.na(LogValueInterp)]$timePointYears))
    
    # #-- ARIMA forescast ----
    
    varselected <- names(bestmod$coef)[names(bestmod$coef) %in% names(xreg_comint)]
    
    #-- Kalman ----
    
    predKalm <- try(na.kalman(x = ppseries, model = bestmod$model,
                              xreg = xreg_comintsig[, varselected, with = F]))

    # If auto.arima does not work then splines
    if(inherits(predKalm, 'try-error') | all(arimaorder(bestmod) == 0) | predKalm[which(is.na(ppseries))] == 0 | 
       predKalm[which(is.na(ppseries))] > (median(ppseries, na.rm = T) + 2*sd(ppseries, na.rm = T)) | 
       predKalm[which(is.na(ppseries))] < (median(ppseries, na.rm = T) - 2*sd(ppseries, na.rm = T)) ){
      
      predKalm <- try(na.kalman(x = ppseries, model = 'auto.arima',
                                xreg = xreg_comintsig[, varselected, with = F]))
      
      if(inherits(predKalm, 'try-error') | predKalm[which(is.na(ppseries))] == 0 | 
         predKalm[which(is.na(ppseries))] > (median(ppseries, na.rm = T) + 2*sd(ppseries, na.rm = T)) | 
         predKalm[which(is.na(ppseries))] < (median(ppseries, na.rm = T) - 2*sd(ppseries, na.rm = T))){
        # predKalm <- na.interp(x = ppseries)
        splinemod <- smooth.spline(x = series_comint[!is.na(LogValueInterp)]$timePointYears, 
                                   y = series_comint[!is.na(LogValueInterp)]$LogValueInterp, all.knots = T)
        
        predKalm <- series_comint[is.na(LogValueInterp),
                                  LogValueInterp := predict(splinemod,
                                                            as.numeric(series_comint$timePointYears))$y[which(is.na(ppseries))]]$LogValueInterp
      }
    }
    
    series_comint[, ValueInterp := exp(predKalm)]
    series2bind <- series_comint[, c("geographicAreaM49",
                                     "measuredItemCPC",
                                     "timePointYears",
                                     "Value",
                                     "flagObservationStatus",
                                     "flagMethod", 
                                     "ValueInterp"), with = F]
    setcolorder(series2bind, c("geographicAreaM49",
                               "measuredItemCPC",
                               "timePointYears",
                               "Value",
                               "flagObservationStatus",
                               "flagMethod", 
                               "ValueInterp"))
    series2bind[, ValueInterp := as.numeric(ValueInterp)]
    interpolated <- rbind(interpolated, series2bind)
    
  }
}

names(interpolated) <- tolower(names(interpolated))
setnames(interpolated, 'valueinterp', 'interpolation')
interpolated[, selected := as.logical(0)]

chngsint <- Changeset('interpolation_annual_prices')
currentInt <- ReadDatatable('interpolation_annual_prices', readOnly = FALSE)

if(nrow(currentInt[geographicaream49 %in% sessionCountry])>0){
  AddDeletions(chngsint,currentInt[geographicaream49 %in% sessionCountry])
  Finalize(chngsint)
}
AddInsertions(chngsint, interpolated)
Finalize(chngsint)

message('Producer prices imputation and interpolation plugin completed')


from = "sws@fao.org"
to = swsContext.userEmail
subject = "PP imputation and interpolation plug-in has correctly run"
body = list('The plugin has correctly run. The following messages have been returned:')
sendmailR::sendmail(from = from, to = to, subject = subject, msg = body)
paste0("Email sent to ", swsContext.userEmail)



```


## Shiny application: imputation validation

The tab 'Missing data imputation' of the shiny application is dedicated to the review and the choice of the value to impute to missing values.
The structure is similar to the 'Outlier validation' tab. The shiny automatically provides in the 'Value to review' box the series that need imputation. ONce the user select the series the sub-tab 'Plugin results' show the graph with the series up to 't-1' and, in different colors, the values estimated at time 't' according to the different methods. Each point of the time series has attached metadata the user can check by pointing to each point in the graph.

On the right side the series with the annual growth rate is displayed in SLC (figure \@ref(fig:shinyImp1)).


```{r  shinyImp1, echo=FALSE, out.width="100%", fig.cap='Shiny app validation of imputation.'}
knitr::include_graphics("img/shinyImp1.png")
```

A table with additional information is also available at the bottom of the tab (figure \@ref(fig:shinyImp1)). The table shows all prices for the selected countries and all the prices for the selected commodity across the world. The user can sort and filter data according to the different variables displyed.


```{r  shinyImp2, echo=FALSE, out.width="100%", fig.cap='Shiny app validation of imputation additional information.'}
knitr::include_graphics("img/shinyImp2.png")
```


To impute the chosen value the user must select the value in the blue list of approaches and validate the choice with the green button 'Validate'.
If none of the approaches proposed is satisfactory the user can click the red button 'Refuse', insert a manual value and 'Impute' it through the button (figure \@ref(fig:shinyImp3)).



```{r  shinyImp3, echo=FALSE, out.width="100%", fig.cap='Shiny app manual imputation.'}
knitr::include_graphics("img/shinyImp3.png")
```

If all or a group of products has to be imputed with the same approach 'as a bulk', the 'Bulk imputation' sub-tab where the user can select what series impute with the same approach. As shown in figure \@ref(fig:shinyBulk1) the user selects the country, the commodity group(s) and the method to use. Then click the green button 'Validate' to save the change into SWS.

```{r  shinyBulk1, echo=FALSE, out.width="100%", fig.cap='Shiny app bulk imputation tab.'}
knitr::include_graphics("img/shinyBulk1.png")
```

##  Shiny application: interpolation

With the same structure of imputation tab there is the 'Interpolation' tab where the user can look all the series for which an interpolation has been made by the plugin. The list therefore includes all series that have an official value at the latest year 't' but have imputed data at time 't-1'. In this way the user can review the previous imputation and check if it is in line with the latest official data.
Depending on the expert assessment, the interpolation proposed by the plugin can be accepted (green button) or refused (red button). Each assessment is saved into SWS and will be transferred to the dataset at the end of the process.

```{r  shinyInterp, echo=FALSE, out.width="100%", fig.cap='Shiny app interpolation tab.'}
knitr::include_graphics("img/shinyInterp.png")
```


##  Shiny application: series revision

In case there is need of a broader revision of the series, the 'Series revision' tab. Here the user needs to select manually the series to visualize (country, start year, end year and commodity). Since this imputation is performed directly ni the shiny application and the new values need to be overwritten directly in the dataset, it is essential to connect the shiny to the session the user wish to write on. 
This session has to be from the 'Annual Producer Prices (Preparation)' dataset (_annual_producer_prices_prep_). The user can get the token through SWS with the 'get_Token' plugin as in previous steps, and paste it into the top right box of the tab. Once the token is saved and the series is selected, imputation according all the available methods is displayed in the graphs of the page. As in the 'Missing data imputation' tab the user can select the approach to impute and 'Validate' the series (figure \@ref(fig:shinySeries1)).


```{r  shinySeries1, echo=FALSE, out.width="100%", fig.cap='Shiny app series revision tab.'}
knitr::include_graphics("img/shinySeries1.png")
```



## Plugin: pp_save_validated_data {#validateimputation}

The validation of the imputation process from the shiny app involve only a modification in the datatable 'imputation_annual_prices' and 'interpolation_annual_prices'. In order to transfer the choices to the dataset and complete the time series into the dataset the plugin 'pp_save_validated_data' must run.
From a session of the dataset 'Annual Producer Prices (Validation)' ('annual_producer_prices_validation') the user can run the plugin as described in figure \@ref(fig:save_valid_data). No manual parameter is required.


```{r  save_valid_data, echo=FALSE, out.width="100%", fig.cap="Run 'pp_save_validated_data' plugin."}
knitr::include_graphics("img/save_valid_data.png")
```


The plugin reads from the imputation and interpolation datatables, it assigns the appropriate flag depending on the chosen estimation approach and attach metadata to each figure about the approach.
Because all the imputation is performed considering prices in SLC, the plugin has to convert values in LCU and USD. The approach is the same as the one in 'pp_fromQuest2Prep' plugin.


### Plugin code

```
# Plugin to transfer from imputed DT to dataset

# -- Load Packages ----

suppressMessages({
  library(data.table)
  library(DT)
  library(forecast)
  library(tseries)
  library(faosws)
  library(faoswsFlag)
  library(faoswsProcessing)
  library(faoswsUtil)
  library(faoswsImputation)
  library(imputeTS)
  library(ggplot2)
  library(sendmailR)
})

# -- Token QA ----

if(CheckDebug()){
  library(faoswsModules)
  SETTINGS = ReadSettings("sws.yml")
  R_SWS_SHARE_PATH = SETTINGS[["share"]]
  SetClientFiles(SETTINGS[["certdir"]])
  GetTestEnvironment(baseUrl = SETTINGS[["server"]],
                     token = '4c304ada-522c-4110-bac6-34a3bc0703e8')  #SETTINGS[["token"]])#'4e9d9a2e-5258-48b6-974f-3656d1af8217')
}


imputations <- ReadDatatable('imputation_annual_prices')

interpolations0 <- ReadDatatable('interpolation_annual_prices')
# Take only what has been interpolated
interpolations <- interpolations0[!flagobservationstatus %in% c('', 'X')]
interpolations <- interpolations[selected == TRUE]

if(interpolations[,.N] == 0){
  interpolations <- data.table()
  msg <- 'No interpolation validation has been found.'
} else {
  msg <- ''
}
interpolations <- interpolations[timepointyears >= (max(as.numeric(interpolations$timepointyears))-3)]
interpolations[,selected := NULL]
interpolations[,interpolation := NULL]
interpolations[,approach := 'Interpolation']
setnames(interpolations, 'value', 'estimation')

# imputations[approach == 'ARIMAX', selected := TRUE]

approaches <- ReadDatatable('method_flag_link')
if(any(!unique(imputations$approach) %in% approaches$method)){
  message(paste("Please update the 'method_flag_link' datatable. These methods are not included: ",
                paste(imputations$approach[!unique(imputations$approach) %in% approaches$method], collapse = ', '), sep = '' ))
}  

validations <- imputations[selected == TRUE]

validations <- merge(validations, approaches, by.x = 'approach', by.y = 'method', all.x = T)

validations[ , selected := NULL]

validations <- rbind(validations, interpolations)

validations[ , measuredElement := '5531']

setnames(validations, c("geographicaream49", "measureditemcpc", "timepointyears", "estimation", "approach"),
         c("geographicAreaM49", "measuredItemCPC", "timePointYears", "ValueSLC", "Metadata_Value"))

includemetadata <- copy(validations[,c("geographicAreaM49", "measuredItemCPC", "timePointYears", "Metadata_Value"), with = F])
includemetadata[,Metadata:="GENERAL"]
includemetadata[,Metadata_Element:="COMMENT"]
includemetadata[,Metadata_Language:="en"]

includemetadata[, measuredElement := '5530']
includemetadata1 <- copy(includemetadata)[, measuredElement := '5531']
includemetadata2 <- copy(includemetadata)[, measuredElement := '5532']

includemetadata <- rbind(includemetadata, includemetadata1, includemetadata2)


validations[, Value := ValueSLC]
validations[,Metadata_Value := NULL]
# get appropriate shape and flags (USD and SLC calculated, 'i')
pper <- melt(validations, measure.vars = c('Value', 'ValueSLC'),
             value.name = 'Value')

pper[variable == 'Value', c('measuredElement') := list('5530')]
pper[ , c('variable')] <- NULL


if(any(pper$flagObservationStatus == 'B')){
  
  geotimecomb <- unique(pper[flagObservationStatus == 'B', .(geographicAreaM49, timePointYears, from_currency)])
  
  # Get datatable with conversion rates 
  # If change of currency (the datatable has to be updated)
  conv_rates <- ReadDatatable('currency_changes')
  
  conv_rates_needed <- merge(conv_rates, geotimecomb, by.x  = 'new_currency_code',
                             by.y = 'from_currency')
  
  slcval <- merge(validations, conv_rates_needed, by = 'geographicAreaM49', 
                  all.x = T, suffixes = c('', '_change'))
  
  slcval[measuredElement == '5530' & timePointYears < timePointYears_change, c('Value',
                                                                               'flagObservationStatus', 
                                                                               'flagMethod'):= list(Value*exchange_rate,
                                                                                                    flagObservationStatus,
                                                                                                    'i')]
  names(slcval)
  slcval[ , c("new_currency_code",    
              "old_currency_code",
              "exchange_rate",
              "timePointYears_change")] <- NULL
  
  slcquest <- merge(pper, conv_rates_needed,  by = 'geographicAreaM49',
                    all.x = T, suffixes = c('', '_change'))
  
  slcquest[measuredElement == '5530' & timePointYears < timePointYears_change, c('Value',
                                                                                 'flagObservationStatus', 
                                                                                 'flagMethod'):= list(Value*exchange_rate,
                                                                                                      flagObservationStatus,
                                                                                                      'i')]
  slcquest[ , c("new_currency_code",    
                "old_currency_code",
                "exchange_rate",
                "timePointYears_change")] <- NULL
  
  
  
} else {
  # slcval <- val_price
  datalcu <- pper
}


#-- USD conversion ----

# Get country-currency datatatble ADD withdraw year/effective change in series
lcu_2_m49 <- ReadDatatable('lcu_2_m49')
lcu_2_m49[start_year_iso == '', start_year_iso := '1900']
lcu_2_m49[end_year_iso == '', end_year_iso := '9999']

# Pull exchange rates dataset

erKey = DatasetKey(
  domain = 'common',
  dataset = 'exchange_rates_annual',
  dimensions = list(
    Dimension(name = 'geographicAreaM49',
              keys = GetCodeList('common', 'exchange_rates_annual', 'geographicAreaM49')[code %in% unique(validations$geographicAreaM49), code]),
    Dimension(name = "from_currency",
              keys = GetCodeList('common', 'exchange_rates_annual', 'from_currency')[code != 'ECU' & endDate >= '1991-01-01' | 
                                                                                       is.na(endDate), code]),
    Dimension(name = "to_currency", 
              keys = GetCodeList('common', 'exchange_rates_annual', 'to_currency')[code == 'USD', code]),
    Dimension(name = 'measuredElement',
              keys = 'LCU'),
    Dimension(name = "timePointYears", 
              keys = GetCodeList('common', 'exchange_rates_annual', 'timePointYears')[code %in% unique(validations$timePointYears), code]))
  
)

erdt <- GetData(erKey, flags = F) 

erdt[,c('measuredElement', 'to_currency')] <- NULL

# # check on currency
# if(!all(erdt$from_currency %in% lcu_2_m49$code_iso)){
#   stop(paste('Missing countey-currency correspondence: ', 
#              unique(erdt[!from_currency %in% lcu_2_m49$code_iso]$from_currency),
#              'not in the lcu_2_m49 datatble. Please update it.'))
# }

# Start conversion into USD and SLC merging with XR

## FIX DUPLICATES!!!!!!

pper0 <- merge(datalcu, erdt[!geographicAreaM49 %in% c('233','428','440')], by = c('geographicAreaM49', 'timePointYears'), all.x = T,
               suffixes = c('', '_er'))

pper0[measuredElement == '5530', ValueUSD := Value/Value_er]
#### erdt[duplicated(erdt[,.( geographicAreaM49, timePointYears)])] !!!!!!!

pper0[, c("Value_er")] <- NULL

pper2 <- melt(pper0, measure.vars = c('Value', 'ValueUSD'),
              value.name = 'Value')

pper2[variable == 'ValueUSD', c('measuredElement') := list('5532')]
pper2[ , c('variable', 'from_currency')] <- NULL

pper3 <- pper2[ !is.na(Value)]


SaveData(domain = 'prod_prices', dataset = 'annual_producer_prices_validation', data = pper3,
         metadata = includemetadata, waitTimeout = Inf)


from = "sws@fao.org"
to = swsContext.userEmail
subject = "PP Validation plug-in has correctly run"
body = list('The plugin has correctly run. The following messages have been returned:',msg)
sendmailR::sendmail(from = from, to = to, subject = subject, msg = body)
paste0("Email sent to ", swsContext.userEmail)

```
